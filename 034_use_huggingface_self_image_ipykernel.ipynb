{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e30284-0009-4f41-a127-b2310b542043",
   "metadata": {},
   "source": [
    "## 034. DEMO huggingface 使用專用 library folder 之 image容器 kernel\n",
    "<span style=\"color:red\">Change to Default kernel:   Image_S_home_pytorch_2.1.0-cuda11.8-cudnn8-devel</span>.\n",
    "\n",
    "Model: https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ\n",
    "\n",
    "ATLAS: https://atlas.nomic.ai/map/c1b88b47-2d9b-47e0-9002-b80766792582/2560fd25-52fe-42f1-a58f-ff5eccc890d2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c0ade8-71ef-4f04-8205-962e78a47788",
   "metadata": {},
   "source": [
    "### TRY IT\n",
    "1. Lauch  a new notebook tab\n",
    "2. Change to kernel:  Image_G_pytorch_2.1.0-cuda11.8-cudnn8-devel\n",
    "3. In first cell add content below\n",
    "\n",
    "```\n",
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin:/usr/ubuntu_bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f85861f-0081-4292-a324-1ca99e02f62f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin:/usr/ubuntu_bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b7114e-8703-439b-aba0-31ab34e28344",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install huggingface-hub hf_transfer -q\n",
    "!pip3 install transformers optimum -q\n",
    "!pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  -q # Use cu117 if on CUDA 11.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8df95223-f119-4342-ba73-e6f48eca8068",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "downloading https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ/resolve/bbedaae2b7aa8836c21fe27d21ea884bc346a89a/.gitattributes to /home/g00cjz00/.cache/huggingface/hub/tmplvnayiua\n",
      ".gitattributes: 100%|██████████| 1.52k/1.52k [00:00<00:00, 6.49MB/s]\n",
      "downloading https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ/resolve/bbedaae2b7aa8836c21fe27d21ea884bc346a89a/README.md to /home/g00cjz00/.cache/huggingface/hub/tmpuh18g2d_\n",
      "README.md: 100%|██████████| 22.5k/22.5k [00:00<00:00, 66.5MB/s]\n",
      "downloading https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ/resolve/bbedaae2b7aa8836c21fe27d21ea884bc346a89a/added_tokens.json to /home/g00cjz00/.cache/huggingface/hub/tmpzr6mfn6l\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 216kB/s]\n",
      "downloading https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ/resolve/bbedaae2b7aa8836c21fe27d21ea884bc346a89a/config.json to /home/g00cjz00/.cache/huggingface/hub/tmpkx_vmraa\n",
      "config.json: 100%|██████████| 1.32k/1.32k [00:00<00:00, 5.53MB/s]\n",
      "downloading https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ/resolve/bbedaae2b7aa8836c21fe27d21ea884bc346a89a/generation_config.json to /home/g00cjz00/.cache/huggingface/hub/tmp7jzrctkb\n",
      "generation_config.json: 100%|██████████| 120/120 [00:00<00:00, 466kB/s]\n",
      "downloading https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ/resolve/bbedaae2b7aa8836c21fe27d21ea884bc346a89a/model.safetensors to /home/g00cjz00/.cache/huggingface/hub/tmp0py4lr3c\n",
      "model.safetensors: 100%|█████████▉| 4.16G/4.16G [00:19<00:00, 212MB/s] \n",
      "downloading https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ/resolve/bbedaae2b7aa8836c21fe27d21ea884bc346a89a/quantize_config.json to /home/g00cjz00/.cache/huggingface/hub/tmpr8gr60dd\n",
      "quantize_config.json: 100%|██████████| 134/134 [00:00<00:00, 535kB/s]\n",
      "downloading https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ/resolve/bbedaae2b7aa8836c21fe27d21ea884bc346a89a/special_tokens_map.json to /home/g00cjz00/.cache/huggingface/hub/tmplpmoty7s\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 281kB/s]\n",
      "downloading https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ/resolve/bbedaae2b7aa8836c21fe27d21ea884bc346a89a/tokenizer.json to /home/g00cjz00/.cache/huggingface/hub/tmpe0_k99ke\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 1.83MB/s]\n",
      "downloading https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ/resolve/bbedaae2b7aa8836c21fe27d21ea884bc346a89a/tokenizer.model to /home/g00cjz00/.cache/huggingface/hub/tmpxad00ell\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 639kB/s]\n",
      "downloading https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GPTQ/resolve/bbedaae2b7aa8836c21fe27d21ea884bc346a89a/tokenizer_config.json to /home/g00cjz00/.cache/huggingface/hub/tmpg6gg5lss\n",
      "tokenizer_config.json: 100%|██████████| 1.69k/1.69k [00:00<00:00, 7.07MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/g00cjz00/github/hpc_notebook_kernel/Mistral-7B-OpenOrca-GPTQ\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# How to download, including from branches\n",
    "mkdir -p Mistral-7B-OpenOrca-GPTQ\n",
    "HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/Mistral-7B-OpenOrca-GPTQ --local-dir Mistral-7B-OpenOrca-GPTQ --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24552997-84b9-4411-9bcc-839cc75800d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "MODEL_ID = \"./Mistral-7B-OpenOrca-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad5959f2-a7c1-4f29-9edf-171ea0ab9fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g00cjz00/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<|im_end|>\n",
      "<|im_start|>user\n",
      "Tell me about AI<|im_end|>\n",
      "<|im_start|>assistant\n",
      " Artificial Intelligence (AI) is a branch of computer science focused on creating intelligent machines capable of performing tasks that would typically require human intelligence, such as learning, reasoning, and problem-solving. AI systems can be trained using large datasets and sophisticated algorithms to recognize patterns, identify objects, understand languages, predict outcomes, and even generate creative content.\n",
      "\n",
      "There are various types of AI, including:\n",
      "\n",
      "1. Narrow AI (Weak AI): This type of AI is designed for specific tasks and has limited capabilities compared to general AI. Examples of narrow AI include facial recognition software, spam filters, and self-driving cars.\n",
      "\n",
      "2. General AI (Strong AI): This is a hypothetical form of artificial intelligence that would be able to perform any intellectual task that a human can. It has the potential to become self-aware and possess consciousness, but it doesn't currently exist in practice.\n",
      "\n",
      "3. Superintelligence: This is a concept referring to an AI far surpassing the cognitive abilities of a human. It is highly speculative and raises concerns about its potential impact on society, ethics, and safety.\n",
      "\n",
      "AI has numerous applications in various fields, such as medicine, finance, transportation, entertainment, and more. Some examples include AI-powered medical diagnoses, personalized advertising, smart traffic management, and virtual assistants like Siri or Alexa. However, AI also brings up questions about privacy, job displacement, and ethical implications related to automation and decision-making. Overall, AI aims to improve efficiency, productivity, and the overall quality of life for humans.\n"
     ]
    }
   ],
   "source": [
    "system_message=\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''<|im_start|>system\n",
    "{system_message}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "'''\n",
    "print(pipe(prompt_template)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387b6e88-f5b3-45e7-8c8b-fafcac54d79b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Image_S_home_pytorch_2.1.0-cuda11.8-cudnn8-devel",
   "language": "python",
   "name": "s_home_pytorch_2.1.0-cuda11.8-cudnn8-devel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
