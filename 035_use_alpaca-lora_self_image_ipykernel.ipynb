{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e30284-0009-4f41-a127-b2310b542043",
   "metadata": {},
   "source": [
    "## 035. DEMO alpaca-lora 使用專用 library folder 之 image容器 kernel\n",
    "<span style=\"color:red\">Change to Default kernel:   Image_S_home_alpaca-lora_latest</span>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c0ade8-71ef-4f04-8205-962e78a47788",
   "metadata": {},
   "source": [
    "### TRY IT\n",
    "1. Lauch  a new notebook tab\n",
    "2. Change to kernel:  Image_S_home_pytorch_2.1.0-cuda11.8-cudnn8-devel\n",
    "3. In first cell add content below\n",
    "\n",
    "```\n",
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin:/usr/ubuntu_bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f85861f-0081-4292-a324-1ca99e02f62f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin:/usr/ubuntu_bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f262aef-48d8-4fe1-b0cd-1b14954ecc41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -rf /work/u00cjz00/slurm_jobs/demo/alpaca-lora ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9dbc656-1871-43f5-8b3f-002d1e3484d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp finetune.py alpaca-lora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "387b6e88-f5b3-45e7-8c8b-fafcac54d79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/g00cjz00/github/hpc_notebook_kernel/alpaca-lora\n"
     ]
    }
   ],
   "source": [
    "%cd alpaca-lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "483a616e-20ab-46a6-ae70-8b51ae18353f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt -q\n",
    "!pip install protobuf install accelerate==0.24.1 bitsandbytes transformers accelerate bitsandbytes bitsandbytes==0.41 scipy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3856936-5573-4936-af5d-5f7c351bbb3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Alpaca-LoRA model with params:\n",
      "base_model: /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf\n",
      "data_path: /work/u00cjz00/slurm_jobs/github/dataset/school_math_30000.json\n",
      "output_dir: ./lora-alpaca\n",
      "batch_size: 128\n",
      "micro_batch_size: 4\n",
      "num_epochs: 3\n",
      "learning_rate: 0.0001\n",
      "cutoff_len: 512\n",
      "val_set_size: 2000\n",
      "lora_r: 8\n",
      "lora_alpha: 16\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: ['q_proj', 'v_proj']\n",
      "train_on_inputs: True\n",
      "add_eos_token: False\n",
      "group_by_length: True\n",
      "wandb_project: \n",
      "wandb_run_name: \n",
      "wandb_watch: \n",
      "wandb_log_model: \n",
      "resume_from_checkpoint: False\n",
      "prompt template: alpaca\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:23<00:00, 11.87s/it]\n",
      "/home/g00cjz00/.local/lib/python3.10/site-packages/peft/utils/other.py:136: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "Downloading and preparing dataset json/default to /home/g00cjz00/.cache/huggingface/datasets/json/default-6462863a4538fd13/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 3339.41it/s]\n",
      "Extracting data files: 100%|██████████████████████| 1/1 [00:00<00:00, 11.20it/s]\n",
      "Dataset json downloaded and prepared to /home/g00cjz00/.cache/huggingface/datasets/json/default-6462863a4538fd13/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 50.12it/s]\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "  0%|▏                                       | 3/654 [03:22<12:06:51, 66.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 finetune.py \\\n",
    "    --base_model '/work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf' \\\n",
    "    --data_path '/work/u00cjz00/slurm_jobs/github/dataset/school_math_30000.json' \\\n",
    "    --output_dir './lora-alpaca' \\\n",
    "    --batch_size 128 \\\n",
    "    --micro_batch_size 4 \\\n",
    "    --num_epochs 3 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --cutoff_len 512 \\\n",
    "    --val_set_size 2000 \\\n",
    "    --lora_r 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --lora_target_modules '[q_proj,v_proj]' \\\n",
    "    --train_on_inputs \\\n",
    "    --group_by_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0606ad-b716-415d-a600-6d245df21709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#若使用單台機器多 GPU ，world_size 表示使用的 GPU 數量\n",
    "#若使用多台機器多 GPU ，world_size 表示使用的機器數量\n",
    "\n",
    "!WORLD_SIZE=2 CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 --nnodes=2 finetune.py \\\n",
    "    --base_model '/work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf' \\\n",
    "    --data_path '/work/u00cjz00/slurm_jobs/github/dataset/school_math_30000.json' \\\n",
    "    --output_dir './lora-alpaca' \\\n",
    "    --batch_size 128 \\\n",
    "    --micro_batch_size 4 \\\n",
    "    --num_epochs 3 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --cutoff_len 512 \\\n",
    "    --val_set_size 2000 \\\n",
    "    --lora_r 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --lora_target_modules '[q_proj,v_proj]' \\\n",
    "    --train_on_inputs \\\n",
    "    --group_by_length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Image_S_home_alpaca-lora_latest",
   "language": "python",
   "name": "s_home_alpaca-lora_latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
